{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zulema Perez\n",
    "\n",
    "### CPSC 483\n",
    "\n",
    "### Project 3: Regularization, Cross-Validation, and Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "\n",
    "Load and examine the Boston dataset's features, target values, and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "dataset_boston = datasets.load_boston()\n",
    "\n",
    "print(dataset_boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "\n",
    "Save CRIM as the new targhet value t, and drop the column CRIM from X.\n",
    "\n",
    "Add the target value MEDV to X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Independent variables (i.e features)\n",
    "df_boston_features = pd.DataFrame(data = dataset_boston.data, columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependent variables (i.e. target)\n",
    "df_boston_target = pd.DataFrame(data = dataset_boston.target, columns = ['MEDV'])\n",
    "\n",
    "target = df_boston_features[['CRIM']]\n",
    "\n",
    "features = df_boston_features.drop(['CRIM'], axis = 1)\n",
    "features['MEDV'] = df_boston_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  PTRATIO       B  \\\n",
       "0  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0     15.3  396.90   \n",
       "1   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0     17.8  396.90   \n",
       "2   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0     17.8  392.83   \n",
       "3   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0     18.7  394.63   \n",
       "4   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0     18.7  396.90   \n",
       "\n",
       "   LSTAT  MEDV  \n",
       "0   4.98  24.0  \n",
       "1   9.14  21.6  \n",
       "2   4.03  34.7  \n",
       "3   2.94  33.4  \n",
       "4   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM\n",
       "0  0.00632\n",
       "1  0.02731\n",
       "2  0.02729\n",
       "3  0.03237\n",
       "4  0.06905"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3\n",
    "\n",
    "Split the features and target values into separate training and test sets. Use 80% of the original data as a training set, and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(404, 1)\n",
      "(102, 13)\n",
      "(102, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Shuffle and split the features and values into separate training and test sets.\n",
    "# Use 80% of original data as a training set and 20% for testing.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4\n",
    "\n",
    "Create and fit() an sklearn.linear_model.LinearRegression to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression score: \n",
      "0.4568543784486474\n",
      "\n",
      "Regression coefficients: \n",
      "[[ 3.92409500e-02 -6.09254350e-02 -9.19332284e-01 -8.15174234e+00\n",
      "   1.18748979e+00 -1.48901554e-02 -9.53485729e-01  5.70760115e-01\n",
      "  -3.68154258e-03 -2.81162432e-01  1.05491408e-03  1.97392423e-01\n",
      "  -2.21757855e-01]]\n",
      "\n",
      "Regression intercept: \n",
      "[8.77038623]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_mod = LinearRegression().fit(X_train,y_train)\n",
    "\n",
    "print()\n",
    "print('Regression score: ')\n",
    "print(lin_mod.score(X_train,y_train))\n",
    "print()\n",
    "print('Regression coefficients: ')\n",
    "print(lin_mod.coef_)\n",
    "print()\n",
    "print('Regression intercept: ')\n",
    "print(lin_mod.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 5\n",
    "\n",
    "Use the predict() method of the model to find the response for each value in the test set, and sklearn.metrics.mean_squared_error(), to find the training and test MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model Training Set MSE: \n",
      "35.88138733484209\n",
      "\n",
      "Linear Model Test Set MSE: \n",
      "61.40060840084935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "y_train_predict = lin_mod.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, y_train_predict)\n",
    "\n",
    "\n",
    "y_test_predict = lin_mod.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_predict)\n",
    "\n",
    "print('Linear Model Training Set MSE: ')\n",
    "print(train_mse)\n",
    "print()\n",
    "print('Linear Model Test Set MSE: ')\n",
    "print(test_mse)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 6\n",
    "\n",
    "By itself, the MSE doesn‚Äôt tell us much. Use the score() method of the model to find the R2 values for the training and test data.\n",
    "R2, the coefficient of determination, measures the proportion of variability in the target t that can be explained using the features in X. A value near 1 indicates that most of the variability in the response has been explained by the regression, while a value near 0 indicates that the regression does not explain much of the variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model Training Set R2 Score: \n",
      "0.4568543784486474\n",
      "\n",
      "Linear Model Test Set R2 Score: \n",
      "0.4075231880832223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r2_train_mod = r2_score(y_train, y_train_predict)\n",
    "r2_test_mod = r2_score(y_test, y_test_predict)\n",
    "\n",
    "print('Linear Model Training Set R2 Score: ')\n",
    "print(r2_train_mod)\n",
    "print()\n",
    "print('Linear Model Test Set R2 Score: ')\n",
    "print(r2_test_mod)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*$ R^2 $*, The *coefficient of determination*, measures the proportion of variability in the target ***t*** that can be explained using the features in ***X***. A value near 1 indicates  that most of the variability in the response has been explained by the regression, while a value near 0 indicates that the regression does not explain much of the variability. See Section 3.1.3 of *An Introduction to Statistical Learning* for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Given the $ R^2$ scores, how well did our model do?***\n",
    "\n",
    "It seems as if our model did an average job of explaining variability. Our regression only explains about 46% of the variability for our training data, and 41% of the variability in our test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 7\n",
    "\n",
    "Let‚Äôs see if we can fit the data better with a more flexible model. Scikit-learn can construct polynomial features for us using sklearn.preprocessing.PolynomialFeatures (though note that this includes interaction features as well; you saw in Project 2 that purely polynomial features can easily be constructed using numpy.hstack()).\n",
    "Add degree-2 polynomial features, then fit a new linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quadratic Model Training Set MSE: \n",
      "21.145710953531\n",
      "\n",
      "Quadratic Model Test Set MSE: \n",
      "59.96156604746587\n",
      "\n",
      "---------------------------------------\n",
      "\n",
      "Quadratic Model Training Set R2 Score: \n",
      "0.6799120331713511\n",
      "\n",
      "Quadratic Model Test Set R2 Score: \n",
      "0.4214090313663984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "\n",
    "X2_train = poly.fit_transform(X_train)\n",
    "X2_test = poly.fit_transform(X_test)\n",
    "\n",
    "quad_mod = LinearRegression().fit(X2_train[:,1:],y_train)\n",
    "\n",
    "y_train_quad_mod_pred = quad_mod.predict(X2_train[:,1:])\n",
    "\n",
    "train_quad_mod_mse = mean_squared_error(y_train, y_train_quad_mod_pred)\n",
    "\n",
    "y_test_quad_mod_pred = quad_mod.predict(X2_test[:,1:])\n",
    "\n",
    "test_quad_mod_mse = mean_squared_error(y_test, y_test_quad_mod_pred)\n",
    "\n",
    "print()\n",
    "print('Quadratic Model Training Set MSE: ')\n",
    "print(train_quad_mod_mse)\n",
    "print()\n",
    "print('Quadratic Model Test Set MSE: ')\n",
    "print(test_quad_mod_mse)\n",
    "print()\n",
    "print('---------------------------------------')\n",
    "\n",
    "r2_train_quad_mod = r2_score(y_train, y_train_quad_mod_pred)\n",
    "r2_test_quad_mod = r2_score(y_test, y_test_quad_mod_pred)\n",
    "\n",
    "print()\n",
    "print('Quadratic Model Training Set R2 Score: ')\n",
    "print(r2_train_quad_mod)\n",
    "print()\n",
    "print('Quadratic Model Test Set R2 Score: ')\n",
    "print(r2_test_quad_mod)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Compare the training and test MSE and $ R^2$ scores. Do we seem to be overfitting?***\n",
    "\n",
    "It actually doesn't seem to be overfitting the data by increasing the order. We do see improvements on all forms of evaluation. The mean squared errors are reduced significantly for the training data, which is to be expected with higher order polynomial models, but are test mean squared error reduces as well suggesting that this model is a better predictor than a linear alternative. In addition, the coefficients of determination for both the training and test data increase meaning that the regression models explains more of the variation, which is also to be expected for the training data, but for the testing data, it shows that the quadratic model has improved overall compared to the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 8\n",
    "\n",
    "Regularization would allow us to construct a model of intermediate complexity by penalizing large values for the coefficients. Scikit-learn provides this as sklearn.linear_model.Ridge. The parameter alpha corresponds to ùúÜ as shown in the textbook. For now, leave it set to the default value of 1.0, and fit the model to the degree-2 polynomial features. Don‚Äôt forget to normalize your features.\n",
    "Once again, compare the training and test MSE and $ R^2$ scores. Is this model an improvement?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Quadratic Model Training Set MSE: \n",
      "32.56792326452567\n",
      "\n",
      "Ridge Quadratic Model Test Set MSE: \n",
      "57.79026378100839\n",
      "\n",
      "---------------------------------------\n",
      "\n",
      "Ridge Quadratic Model Training Set R2 Score: \n",
      "0.5070111208612387\n",
      "\n",
      "Ridge Quadratic Model Test Set R2 Score: \n",
      "0.4423607169936796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_train = Ridge(alpha = 1.0, normalize = True)\n",
    "\n",
    "ridge_train.fit(X2_train, y_train)\n",
    "\n",
    "y_train_ridge_pred = ridge_train.predict(X2_train)\n",
    "\n",
    "train_ridge_mse = mean_squared_error(y_train, y_train_ridge_pred)\n",
    "\n",
    "y_test_ridge_pred = ridge_train.predict(X2_test)\n",
    "\n",
    "test_ridge_mse = mean_squared_error(y_test, y_test_ridge_pred)\n",
    "\n",
    "print()\n",
    "print('Ridge Quadratic Model Training Set MSE: ')\n",
    "print(train_ridge_mse)\n",
    "print()\n",
    "print('Ridge Quadratic Model Test Set MSE: ')\n",
    "print(test_ridge_mse)\n",
    "print()\n",
    "print('---------------------------------------')\n",
    "\n",
    "r2_train_ridge = r2_score(y_train, y_train_ridge_pred)\n",
    "r2_test_ridge = r2_score(y_test, y_test_ridge_pred)\n",
    "\n",
    "print()\n",
    "print('Ridge Quadratic Model Training Set R2 Score: ')\n",
    "print(r2_train_ridge)\n",
    "print()\n",
    "print('Ridge Quadratic Model Test Set R2 Score: ')\n",
    "print(r2_test_ridge)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Once again, compare the training and test MSE and *$R^2$* scores. Is this model an improvement?***\n",
    "\n",
    "Compared to the previous to analyses, this is the best model so far. The reasons being that even though we see a rise in the training mean sqaured error, the test mean squared error went down suggesting that this model is an even better predictor than the previous one. I similar evaluation of the coefficients of determination can be made as well; even though this model explains less of the variance in the training data, it does a better job of explaing the variance in the test data which means that this model has found a better balance between bias and variation than the previous two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 9\n",
    "\n",
    "We used the default penalty value of 1.0 in the previous experiment, but there‚Äôs no reason to believe that this is optimal. Use sklearn.linear_model.RidgeCV to find an optimal value for alpha. How does this compare to experiment (8)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal value for alpha: \n",
      "0.01\n",
      "\n",
      "RidgeCV Quadratic Model Training Set MSE: \n",
      "26.005425265928906\n",
      "\n",
      "Ridge Quadratic Model Test Set MSE: \n",
      "50.41119717265961\n",
      "\n",
      "---------------------------------------\n",
      "\n",
      "RidgeCV Quadratic Model Training Set R2 Score: \n",
      "0.6063493103552717\n",
      "\n",
      "RidgeCV Quadratic Model Test Set R2 Score: \n",
      "0.5135640156726478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridgecv_train = RidgeCV(alphas = [0.01, 1.0, 10.0], normalize = True)\n",
    "\n",
    "ridgecv_train.fit(X2_train, y_train)\n",
    "\n",
    "print()\n",
    "print('Optimal value for alpha: ')\n",
    "print(ridgecv_train.alpha_)\n",
    "\n",
    "y_train_ridgecv_pred = ridgecv_train.predict(X2_train)\n",
    "train_ridgecv_mse = mean_squared_error(y_train, y_train_ridgecv_pred)\n",
    "\n",
    "y_test_ridgecv_pred = ridgecv_train.predict(X2_test)\n",
    "test_ridgecv_mse = mean_squared_error(y_test, y_test_ridgecv_pred)\n",
    "\n",
    "print()\n",
    "print('RidgeCV Quadratic Model Training Set MSE: ')\n",
    "print(train_ridgecv_mse)\n",
    "print()\n",
    "print('Ridge Quadratic Model Test Set MSE: ')\n",
    "print(test_ridgecv_mse)\n",
    "print()\n",
    "print('---------------------------------------')\n",
    "\n",
    "r2_train_ridgecv = r2_score(y_train, y_train_ridgecv_pred)\n",
    "r2_test_ridgecv = r2_score(y_test, y_test_ridgecv_pred)\n",
    "\n",
    "print()\n",
    "print('RidgeCV Quadratic Model Training Set R2 Score: ')\n",
    "print(r2_train_ridgecv)\n",
    "print()\n",
    "print('RidgeCV Quadratic Model Test Set R2 Score: ')\n",
    "print(r2_test_ridgecv)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***How does this compare to experiment (8)?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually see significant improvements in this model compared to the model in experiment 8. This model has acheived a reduction in both training MSE and test MSE by lowering the penalty value for alpha to 0.01, opposed to the previous value used of 1. Both variations in the training and test data are better explained in this regression model as well with an increase of about 10% for the training data and about 7% for the test data. This is the best model so far out of all models attempted in these series of experiements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
